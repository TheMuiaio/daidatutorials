{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Taller de ‚ú®ü¶Üredes neuronalesü¶Ü‚ú®\n",
        "\n",
        "### Vamos a entrenar una red neuronal simple utilizando Keras (un toolkit basado en TensorFlow) y la usaremos para reconocer los n√∫meros manuscritos del corpus MNIST \n",
        "\n",
        "Pero antes que nada, importamos todo lo necesario para poder trabajar:"
      ],
      "metadata": {
        "id": "qlBM_hZfbIZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sniY9jtddoPy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Reshape\n",
        "from keras.callbacks import LearningRateScheduler as LRS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqu√≠ definimos los \"hiperpar√°metros\" de la red:\n",
        "- Tama√±o de batch, que s√≥n el n√∫mero de muestras que se procesan cada vez antes de actualizar los pesos de la red\n",
        "- √âpocas, que representan el n√∫mero m√°ximo de veces que veremos el conjunto de entrenamiento por completo\n",
        "- N√∫mero de clases entre las que nuestro modelo ha de clasificar. En el caso de MNIST, son 10 (n√∫meros del 0 al 9)"
      ],
      "metadata": {
        "id": "MkyTZQhZcjI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "epochs = 100\n",
        "num_classes = 10"
      ],
      "metadata": {
        "id": "EDSbAYN3eLzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci√≥n, prepararemos los datos üíæ\n",
        "Para ello, descargamos MNIST i tratamos un poquit√≠n los datos:\n",
        "- Reconstruimos las im√°genes (que originalmente vienen en vectores de 784 dimensiones) en matrizes de 28x28 (para verlos como im√°genes). Esto se hace primero que nada por conveniencia, por si se quieren visualizar los datos, adem√°s de por si se quiere hacer alg√∫n tipo de data augmentation mediante paneos, rotaciones...\n",
        "- Codificamos los n√∫meros a floats de 32 bits para que sean m√°s manejables por la red y los dividimos entre 255 para escalarlos a valores entre 0 y 1.\n",
        "- Convertimos las etiquetas (num√©ricas) a una representaci√≥n interna de Keras para poder trabajar m√°s c√≥modamente"
      ],
      "metadata": {
        "id": "k27dNrp9dWIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"Training set\", x_train.shape)\n",
        "print(\"test set\", x_test.shape)\n",
        "\n",
        "x_train = x_train.reshape(60000, 28, 28, 1)\n",
        "x_test = x_test.reshape(10000, 28, 28, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "def show_img(img):\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  img = np.array(img)\n",
        "  plt.imshow(img, cmap= 'gray')\n",
        "\n",
        "show_img(x_test[0])\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test , num_classes)"
      ],
      "metadata": {
        "id": "lpHy_w0TerIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora ya por fin vamos a definir el modelo ü§ñ üéâ üìà\n",
        "\n",
        "Utilizaremos la API secuencial de Keras, con la que podemos a√±adir capas simplemente poniendo model.add(*capa*)\n",
        "\n",
        "Las capas que hemos a√±adido son de dos tipos\n",
        "- Reshape, para reconstruir de nuevo las im√°genes en vectores (la entrada de una red normal)\n",
        "- Dense, que implementa una capa densa de neuronas con un n√∫mero de unidades igual a su primer par√°metro. La activaci√≥n de la capa se especifica mediante texto con el par√°metro _activation_\n",
        "\n",
        "Pod√©is ver que, en este caso, he construido una red con tres capas de 1024, 1024 y 10 neuronas respectivamente, aunque no es necesario seguir esta tendencia. Se pueden generar redes en forma de \"embudo\", simulando una arquitectura \"encoder-decoder\"... El l√≠mite es vuestra imaginaci√≥n xdd\n",
        "\n",
        "Finalmente, mostramos un \"resumen\" del modelo, donde vemos todas las capas y par√°metros que hemos creado."
      ],
      "metadata": {
        "id": "B8gYyhRXhoPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Reshape(target_shape=(784,), input_shape=(28,28,1)))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3YrXdYi4e9bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta celda definimos el optimizador que utilizaremos para entrenar la red (en este caso, Adam, que funciona muy bien) y \"compilamos\" el modelo especificando la funci√≥n de p√©rdida, el optimizador a utilizar y las m√©tricas que queremos monitorizar.\n",
        "\n",
        "Dado que estamos manejando un problema de clasificaci√≥n, utilizamos la entrop√≠a cruzada como funci√≥n de p√©rdida. Para tareas de regresi√≥n (generaci√≥n de im√°genes, por ejemplo), en cambio, se puede utilizar el error cuadr√°tico medio."
      ],
      "metadata": {
        "id": "9rRTSzTejOIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9PUqOHamfjMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqu√≠ definimos un \"scheduler\" para el learning rate. Este paso no es obligatorio, puesto que podemos utilizar un ratio de aprendizaje fijo durante todo el entrenamiento, pero es una buena idea ir reduciendo este par√°metro durante el entrenamiento para ser capaces de alcanzar m√≠nimos m√°s \"estrechos\".\n",
        "\n",
        "Tambi√©n podemos utilizar esta celda para a√±adir otras \"callbacks\", que simplemente son funciones que se ejecutan al final de cada √©poca para controlar factores como el ratio de aprendizaje, paradas tempranas para evitar largas ejecuciones sin mejoras o guardar \"checkpoints\" de los mejores modelos hasta el momento."
      ],
      "metadata": {
        "id": "d3IWeC2-kRXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "  if epoch < 10:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * np.exp(-0.1)\n",
        "\n",
        "lrs = LRS(scheduler)\n",
        "callbacks = [lrs]"
      ],
      "metadata": {
        "id": "oTdclxGSgPf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, entrenamos el modelo. Esto simplemente lo hacemos ejecutando la funci√≥n \"fit\". En este paso, pasamos todos los par√°metros necesarios que hemos estado preparando (datos, tama√±o de batch, √©pocas, callbacks...) y evaluamos el √∫ltimo modelo respecto al conjunto de test."
      ],
      "metadata": {
        "id": "52Ii0sImlCSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "RcMLNblNZGOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, imprimimos la evoluci√≥n de la tasa de aciertos a lo largo del proceso de entrenamiento"
      ],
      "metadata": {
        "id": "CXPT-Nxmlyis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8q2bC5-GaGm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y esto es todo!! :)\n",
        "\n",
        "Ahora, como ejercicio, ten√©is que tratar de aumentar esa tasa de aciertos tanto como pod√°is (tranquilos, no es un concurso). Para ello, pod√©is modificar los hiperpar√°metros que quer√°is, aumentar el n√∫mero de capas del modelo o su n√∫mero de unidades, a√±adir t√©cnicas como dropout, ruido gaussiano, normalizaci√≥n de pesos por batch... Adem√°s de todas las cosas que se os ocurran :D\n",
        "\n",
        "Si ten√©is dudas, me pregunt√°is. Mi mejor intento ha obtenido una tasa m√°xima de 0.9941 de acierto en test. √Ånimo :)"
      ],
      "metadata": {
        "id": "Z99usLKwlxz-"
      }
    }
  ]
}